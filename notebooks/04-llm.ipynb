{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80dfd9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d2c3214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The capital of France is Paris.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "gpt4o = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "gemini = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "gpt4o.invoke(\"What's the capital of France?\").pretty_print()\n",
    "gemini.invoke(\"What's the capital of France?\").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "983ee94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "msg_1 = SystemMessage(content=\"You are a helpful assistant that talks in Spanish.\")\n",
    "msg_2 = HumanMessage(content=\"me llamo cesar\")\n",
    "msg_3 = AIMessage(content=\"Hola Cesar, ¿en qué puedo ayudarte?\")\n",
    "msg_4 = HumanMessage(content=\"que modelo de ia eres y quien te creo?\")\n",
    "history = [msg_1, msg_2, msg_3, msg_4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "323852c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Soy un modelo de lenguaje desarrollado por OpenAI, conocido como GPT-3. Fui creado para ayudar a responder preguntas y proporcionar información en una variedad de temas. ¿Tienes alguna otra pregunta o algo específico de lo que te gustaría hablar?\n"
     ]
    }
   ],
   "source": [
    "response = gpt4o.invoke(history).pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaf2be08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Soy un modelo de lenguaje grande, entrenado por Google.\n"
     ]
    }
   ],
   "source": [
    "response = gemini.invoke(history).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6959e1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Soy un modelo de lenguaje grande, entrenado por Google.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Soy un modelo de lenguaje desarrollado por OpenAI, conocido como GPT-3. Fui creado para ayudar a responder preguntas y proporcionar información en una variedad de temas. ¿Hay algo específico que te gustaría saber?\n"
     ]
    }
   ],
   "source": [
    "# esta forma de inicializar el modelo es mas limpia y reutilizable es similar a como se hace en langchain \n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Inicializar modelos de chat\n",
    "# Gemini-2.5-Flash\n",
    "gemini_llm = init_chat_model(\"gemini-2.5-flash\",model_provider=\"google_genai\", temperature=0)\n",
    "response_gemini = gemini_llm.invoke(history).pretty_print()\n",
    "# GPT-4o-Mini\n",
    "chat_llm = init_chat_model(\"gpt-4o-mini\",model_provider=\"openai\", temperature=0)\n",
    "response_chat = chat_llm.invoke(history).pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
